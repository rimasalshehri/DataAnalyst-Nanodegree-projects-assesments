{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter, Drop Nulls, Dedupe\n",
    "Use `data_08_v2.csv` and `data_18_v2.csv`. You should've created these data files in the previous section: *Renaming Columns*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import pandas and load datasets\n",
    "\n",
    "df_08 = pd.read_csv('data_08_v2.csv')\n",
    "df_18 = pd.read_csv('data_18_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2404, 14)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view dimensions of dataset\n",
    "df_08.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1611, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view dimensions of dataset\n",
    "df_18.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Certification Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_08_filtered = df_08.query(\"cert_region == 'CA'\")\n",
    "df_18_filtered = df_18.query(\"cert_region == 'CA'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CA', 'FA', 'FC'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm only certification region is California\n",
    "df_08['cert_region'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FA', 'CA'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm only certification region is California\n",
    "df_18['cert_region'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop certification region columns form both df_08 and df_18 datasets\n",
    "# you no longer this column since all data should now \n",
    "# be filtered by California\n",
    "df_08.drop('cert_region', axis=1, inplace=True)\n",
    "df_18.drop('cert_region', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2404, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_08.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1611, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_18.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Rows with Missing Values\n",
    "As a reminder, you can use the `.isnull()` method to view how many null values are in each column.  \n",
    "To count every null from every column, sum the values returned from `.isnull()` by using `.sum()` to return a total count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                     0\n",
      "displ                     0\n",
      "cyl                     199\n",
      "trans                   199\n",
      "drive                    93\n",
      "fuel                      0\n",
      "veh_class                 0\n",
      "air_pollution_score       0\n",
      "city_mpg                199\n",
      "hwy_mpg                 199\n",
      "cmb_mpg                 199\n",
      "greenhouse_gas_score    199\n",
      "smartway                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# view missing value count for each feature in 2008\n",
    "missing_values_count = df_08.isnull().sum()\n",
    "print(missing_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                   0\n",
      "displ                   2\n",
      "cyl                     2\n",
      "trans                   0\n",
      "drive                   0\n",
      "fuel                    0\n",
      "veh_class               0\n",
      "air_pollution_score     0\n",
      "city_mpg                0\n",
      "hwy_mpg                 0\n",
      "cmb_mpg                 0\n",
      "greenhouse_gas_score    0\n",
      "smartway                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# view missing value count for each feature in 2018\n",
    "missing_values_count = df_18.isnull().sum()\n",
    "print(missing_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop rows with any null values in both datasets\n",
    "\n",
    "df_08.dropna(inplace=True)\n",
    "df_18.dropna(inplace=True)\n",
    "\n",
    "df_08.dropna( inplace=True)\n",
    "df_18.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checks if any of columns in 2008 have null values - should print False\n",
    "df_08.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checks if any of columns in 2018 have null values - should print False\n",
    "df_18.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dedupe Data\n",
    "Check if there are duplicate values in your DataFrames by using the `.duplicated()` function. Read more on the functionality [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in 2008 dataset: 555\n",
      "Number of duplicates in 2018 dataset: 777\n"
     ]
    }
   ],
   "source": [
    "# print number of duplicates in 2008 and 2018 datasets\n",
    "# remember to use .sum() on the retured values from .duplicated() to count all total duplicates.\n",
    "\n",
    "duplicates_08 = df_08.duplicated().sum()\n",
    "duplicates_18 = df_18.duplicated().sum()\n",
    "\n",
    "print(\"Number of duplicates in 2008 dataset:\", duplicates_08)\n",
    "print(\"Number of duplicates in 2018 dataset:\", duplicates_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop duplicates in both datasets\n",
    "df_08.drop_duplicates(inplace=True)\n",
    "df_18.drop_duplicates(inplace=True)\n",
    "\n",
    "df_08.drop_duplicates( inplace=True)\n",
    "df_18.drop_duplicates( inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in 2008 dataset: 0\n",
      "Number of duplicates in 2018 dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# print number of duplicates again to confirm dedupe - should both be 0\n",
    "duplicates_08 = df_08.duplicated().sum()\n",
    "duplicates_18 = df_18.duplicated().sum()\n",
    "\n",
    "print(\"Number of duplicates in 2008 dataset:\", duplicates_08)\n",
    "print(\"Number of duplicates in 2018 dataset:\", duplicates_18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save progress for the next section\n",
    "df_08.to_csv('data_08_v3.csv', index=False)\n",
    "df_18.to_csv('data_18_v3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
